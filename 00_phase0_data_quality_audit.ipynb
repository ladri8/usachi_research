{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47c1d8dd",
   "metadata": {},
   "source": [
    "# Phase 0: Data Quality & Cleaning Report\n",
    "## Comprehensive Data Audit and Cohort Flow\n",
    "\n",
    "**Purpose:** Track all data cleaning decisions, removals, and transformations from raw data to analysis sample.  \n",
    "**Report Date:** January 19, 2026\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfa7a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from great_tables import GT\n",
    "\n",
    "# Configure for detailed output\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE 0: DATA QUALITY & CLEANING AUDIT\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27ace93",
   "metadata": {},
   "source": [
    "## Step 1: Load Raw Data & Initial Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2ee08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "df_raw = pd.read_csv(\"USACHICTSI7435_DATA_LABELS_2025-12-02_1515.csv\")\n",
    "\n",
    "# Initialize tracking\n",
    "cohort_flow = []\n",
    "\n",
    "# Record initial state\n",
    "n_initial = len(df_raw)\n",
    "n_vars_initial = len(df_raw.columns)\n",
    "\n",
    "cohort_flow.append({\n",
    "    'Stage': '1. Raw data loaded',\n",
    "    'N': n_initial,\n",
    ",\n",
    ",\n",
    "print(f\"-\" * 60)\n",
    "print(f\"Participants: {n_initial}\")\n",
    "print(f\"Variables: {n_vars_initial}\")\n",
    "print(f\"\\nFirst few column names (showing data quality issues):\")\n",
    "print(df_raw.columns[:10].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81682f38",
   "metadata": {},
   "source": [
    "## Step 2: Column Name Cleaning & Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ab952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work with a copy\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Before cleaning - sample column names\n",
    "print(\"\\nüìã COLUMN NAME CLEANING\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"\\nBefore: Sample of original column names:\")\n",
    "print(df.columns[:5].tolist())\n",
    "\n",
    "# Clean column names\n",
    "df.columns = df.columns.astype(str)\n",
    "df.columns = df.columns.str.replace(\"'\", \"\")  # Remove quotes\n",
    "df.columns = df.columns.str.replace('\\xa0', ' ')  # Remove non-breaking spaces\n",
    "df.columns = df.columns.str.strip()  # Remove leading/trailing spaces\n",
    "df.columns = df.columns.str.lower()  # Lowercase for consistency\n",
    "\n",
    "print(f\"\\nAfter: Cleaned column names:\")\n",
    "print(df.columns[:5].tolist())\n",
    "\n",
    "# Rename key variables for easier access\n",
    "rename_dict = {\n",
    "    \"survey date\": \"survey_date\",\n",
    "    \"what is your age?\": \"age\",\n",
    "    \"what is your biological sex?\": \"sex\",\n",
    "    \"what is your household income?\": \"income\",\n",
    "    \"in what year did you or your family arrive to the united states?\": \"year_arrived_us\"\n",
    "}\n",
    "\n",
    "df = df.rename(columns=rename_dict)\n",
    "\n",
    "print(f\"\\nVariables renamed for convenience: {list(rename_dict.keys())}\")\n",
    "print(f\"No rows excluded in this step (data transformation only)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63837388",
   "metadata": {},
   "source": [
    "## Step 3: Variable Selection & Initial Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616089d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables needed for analysis\n",
    "core_cols = [\n",
    "    \"survey_date\", \n",
    "    \"age\", \n",
    "    \"sex\", \n",
    "    \"income\", \n",
    "    \"year_arrived_us\"\n",
    "]\n",
    "\n",
    "cv_cols_original = [\n",
    "    \"have you been diagnosed with any of the following  (choice=heart failure)\",\n",
    "    \"have you been diagnosed with any of the following  (choice=hypertension)\",\n",
    "    \"heart attack (choice=yes)\",\n",
    "    \"stroke (choice=yes)\"\n",
    "]\n",
    "\n",
    "# Check which variables exist\n",
    "all_needed_cols = [col for col in (core_cols + cv_cols_original) if col in df.columns]\n",
    "missing_vars = set(core_cols + cv_cols_original) - set(all_needed_cols)\n",
    "\n",
    "print(\"\\nüîç VARIABLE SELECTION\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Total variables in raw data: {len(df.columns)}\")\n",
    "print(f\"Variables needed: {len(core_cols + cv_cols_original)}\")\n",
    "print(f\"Variables available: {len(all_needed_cols)}\")\n",
    "print(f\"Variables missing: {len(missing_vars)}\")\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"\\n‚ö†Ô∏è  Missing variables: {missing_vars}\")\n",
    "\n",
    "# Select only needed columns\n",
    "df = df[all_needed_cols].copy()\n",
    "\n",
    "print(f\"\\nVariables retained for analysis: {len(df.columns)}\")\n",
    "print(f\"No rows excluded (variable selection only)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d914a915",
   "metadata": {},
   "source": [
    "## Step 4: Data Type Conversion & Initial Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c93cae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîÑ DATA TYPE CONVERSION & VALIDATION\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Track conversions and issues\n",
    "conversion_log = []\n",
    "\n",
    "# 1. Survey Date\n",
    "print(\"\\n1. SURVEY_DATE\")\n",
    "print(f\"   Original type: {df['survey_date'].dtype}\")\n",
    "print(f\"   Sample values: {df['survey_date'].head(3).tolist()}\")\n",
    "df[\"survey_date\"] = pd.to_datetime(df[\"survey_date\"], errors=\"coerce\")\n",
    "invalid_dates = df['survey_date'].isna().sum()\n",
    "print(f\"   After conversion: {df['survey_date'].dtype}\")\n",
    "print(f\"   ‚ö†Ô∏è  Invalid dates converted to NaN: {invalid_dates}\")\n",
    "if invalid_dates > 0:\n",
    "    conversion_log.append(f\"Survey date: {invalid_dates} invalid values\")\n",
    "\n",
    "# 2. Age\n",
    "print(\"\\n2. AGE\")\n",
    "print(f\"   Original type: {df['age'].dtype}\")\n",
    "print(f\"   Sample values: {df['age'].head(3).tolist()}\")\n",
    "df[\"age\"] = pd.to_numeric(df[\"age\"], errors=\"coerce\")\n",
    "invalid_ages = df['age'].isna().sum()\n",
    "print(f\"   After conversion: {df['age'].dtype}\")\n",
    "print(f\"   ‚ö†Ô∏è  Non-numeric values converted to NaN: {invalid_ages}\")\n",
    "if df['age'].notna().any():\n",
    "    print(f\"   Range: {df['age'].min():.0f} - {df['age'].max():.0f}\")\n",
    "\n",
    "# 3. Year Arrived US\n",
    "print(\"\\n3. YEAR_ARRIVED_US\")\n",
    "print(f\"   Original type: {df['year_arrived_us'].dtype}\")\n",
    "print(f\"   Sample values: {df['year_arrived_us'].head(3).tolist()}\")\n",
    "df[\"year_arrived_us\"] = pd.to_numeric(df[\"year_arrived_us\"], errors=\"coerce\")\n",
    "invalid_years = df['year_arrived_us'].isna().sum()\n",
    "print(f\"   After conversion: {df['year_arrived_us'].dtype}\")\n",
    "print(f\"   ‚ö†Ô∏è  Non-numeric values converted to NaN: {invalid_years}\")\n",
    "if df['year_arrived_us'].notna().any():\n",
    "    print(f\"   Range: {df['year_arrived_us'].min():.0f} - {df['year_arrived_us'].max():.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3c7f7a",
   "metadata": {},
   "source": [
    "## Step 5: Outlier Detection & Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b85e36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n‚ö° OUTLIER DETECTION & REMOVAL\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Track removals\n",
    "n_before_outliers = len(df)\n",
    "outlier_exclusions = []\n",
    "\n",
    "# 1. Age outliers\n",
    "print(\"\\n1. AGE OUTLIERS (expected range: 0-120)\")\n",
    "age_invalid = ((df[\"age\"] < 0) | (df[\"age\"] > 120) | df[\"age\"].isna())\n",
    "n_age_invalid = age_invalid.sum()\n",
    "print(f\"   Records with age < 0 or > 120: {n_age_invalid}\")\n",
    "outlier_exclusions.append(('Age out of range', n_age_invalid))\n",
    "\n",
    "# Keep only valid ages for now (we'll use this for filtering)\n",
    "df_valid_age = df[~age_invalid].copy()\n",
    "print(f\"   Remaining records: {len(df_valid_age)}\")\n",
    "\n",
    "# 2. Year arrived outliers  \n",
    "print(\"\\n2. YEAR_ARRIVED_US OUTLIERS (expected range: 1900-2023)\")\n",
    "year_invalid = ((df[\"year_arrived_us\"] < 1900) | (df[\"year_arrived_us\"] > 2023) | df[\"year_arrived_us\"].isna())\n",
    "n_year_invalid = year_invalid.sum()\n",
    "print(f\"   Records with year_arrived_us < 1900 or > 2023: {n_year_invalid}\")\n",
    "outlier_exclusions.append(('Year arrived out of range', n_year_invalid))\n",
    "\n",
    "# Keep only valid years\n",
    "df_valid_year = df[~year_invalid].copy()\n",
    "print(f\"   Remaining records: {len(df_valid_year)}\")\n",
    "\n",
    "# Combined: valid age AND valid year\n",
    "print(\"\\n3. COMBINED FILTERING (valid age AND valid year_arrived)\")\n",
    "valid_mask = (~age_invalid) & (~year_invalid)\n",
    "df = df[valid_mask].copy()\n",
    "n_excluded_outliers = n_before_outliers - len(df)\n",
    "print(f\"   Records excluded: {n_excluded_outliers}\")\n",
    "print(f\"   Records remaining: {len(df)}\")\n",
    "\n",
    "cohort_flow.append({\n",
    "    'Stage': '2. Outliers removed',\n",
    "    'N': len(df),\n",
    "    'Excluded': n_excluded_outliers,\n",
    "    'Reason': 'Age (0-120) or Year arrived (1900-2023) out of range',\n",
    "    'Cumulative_N': len(df)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6331df77",
   "metadata": {},
   "source": [
    "## Step 6: Derived Variables & Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e03440",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîß DERIVED VARIABLES & VALIDATION\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Create arrival date\n",
    "print(\"\\n1. CREATING years_in_us (derived from survey_date and year_arrived_us)\")\n",
    "df[\"arrival_date\"] = pd.to_datetime(\n",
    "    df[\"year_arrived_us\"].astype(\"Int64\").astype(str) + \"-07-01\",\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "df[\"years_in_us\"] = (df[\"survey_date\"] - df[\"arrival_date\"]).dt.days / 365.25\n",
    "\n",
    "# Validation: years_in_us should roughly equal (survey_year - arrival_year)\n",
    "df[\"expected_years\"] = df[\"survey_date\"].dt.year - df[\"year_arrived_us\"]\n",
    "df[\"years_diff\"] = (df[\"years_in_us\"] - df[\"expected_years\"]).abs()\n",
    "\n",
    "# Check for major discrepancies (>1 year difference)\n",
    "large_discrepancies = (df[\"years_diff\"] > 1).sum()\n",
    "print(f\"   Derived years_in_us from arrival_date\")\n",
    "print(f\"   Validation check: Years_in_us vs (survey_year - arrival_year)\")\n",
    "print(f\"   ‚ö†Ô∏è  Records with >1 year discrepancy: {large_discrepancies}\")\n",
    "\n",
    "if df[\"years_in_us\"].notna().any():\n",
    "    print(f\"   Range: {df['years_in_us'].min():.1f} to {df['years_in_us'].max():.1f} years\")\n",
    "    print(f\"   Mean: {df['years_in_us'].mean():.1f} years\")\n",
    "\n",
    "# Apply bounds to years_in_us\n",
    "print(\"\\n2. BOUNDING years_in_us (expected range: 0-120 years)\")\n",
    "years_invalid = ((df[\"years_in_us\"] < 0) | (df[\"years_in_us\"] > 120))\n",
    "n_years_invalid = years_invalid.sum()\n",
    "print(f\"   Records with years_in_us < 0 or > 120: {n_years_invalid}\")\n",
    "df.loc[years_invalid, \"years_in_us\"] = np.nan\n",
    "\n",
    "# Clean up temporary columns\n",
    "df = df.drop(columns=[\"expected_years\", \"years_diff\", \"arrival_date\"])\n",
    "\n",
    "print(f\"\\n‚úì Derived variables created and validated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc83c67",
   "metadata": {},
   "source": [
    "## Step 7: Cardiovascular Outcome Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71195a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüíì CARDIOVASCULAR OUTCOME PROCESSING\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Rename CV variables for convenience\n",
    "cv_rename_map = {\n",
    "    \"have you been diagnosed with any of the following  (choice=heart failure)\": \"dx_hf\",\n",
    "    \"have you been diagnosed with any of the following  (choice=hypertension)\": \"dx_htn\",\n",
    "    \"heart attack (choice=yes)\": \"hx_mi\",\n",
    "    \"stroke (choice=yes)\": \"hx_stroke\",\n",
    "}\n",
    "\n",
    "df = df.rename(columns=cv_rename_map)\n",
    "\n",
    "print(f\"\\nCV variables renamed for convenience\")\n",
    "\n",
    "# Before conversion: examine raw values\n",
    "print(\"\\nüìä RAW VALUES BEFORE CONVERSION:\")\n",
    "for col in [\"dx_hf\", \"dx_htn\", \"hx_mi\", \"hx_stroke\"]:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\n   {col}:\")\n",
    "        print(f\"      Non-null count: {df[col].notna().sum()}\")\n",
    "        print(f\"      Unique values: {df[col].nunique()}\")\n",
    "        print(f\"      Sample values: {df[col].dropna().unique()[:5]}\")\n",
    "\n",
    "# Conversion function\n",
    "def to_binary(x):\n",
    "    if pd.isna(x):\n",
    "        return 0  # NaN/empty = not checked = 0\n",
    "    s = str(x).strip().lower()\n",
    "    \n",
    "    yes_set = {\"yes\", \"y\", \"true\", \"1\", \"1.0\", \"checked\", \"check\", \"x\", \"selected\"}\n",
    "    no_set = {\"no\", \"n\", \"false\", \"0\", \"0.0\", \"unchecked\", \"uncheck\", \"\"}\n",
    "    \n",
    "    if s in yes_set:\n",
    "        return 1\n",
    "    if s in no_set:\n",
    "        return 0\n",
    "    if len(s) > 0:\n",
    "        return 1  # Any other non-empty value = checked\n",
    "    \n",
    "    return 0\n",
    "\n",
    "# Convert CV outcomes to binary\n",
    "cv_outcomes = [\"dx_hf\", \"dx_htn\", \"hx_mi\", \"hx_stroke\"]\n",
    "print(\"\\nüîÑ CONVERTING TO BINARY (0/1):\")\n",
    "for col in cv_outcomes:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(to_binary)\n",
    "        n_positive = df[col].sum()\n",
    "        pct_positive = (df[col].mean() * 100)\n",
    "        print(f\"   {col}: {int(n_positive)} positive ({pct_positive:.1f}%)\")\n",
    "\n",
    "print(\"\\n‚úì CV outcomes converted to binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc3dc44",
   "metadata": {},
   "source": [
    "## Step 8: Create Year of Arrival Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9400a46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìÖ CREATING YEAR OF ARRIVAL BINS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Distribution of year_arrived_us\n",
    "print(f\"\\nDistribution of year_arrived_us (valid records):\")\n",
    "print(df[\"year_arrived_us\"].describe())\n",
    "\n",
    "# Create 3-bin categorization\n",
    "df[\"year_arrived_bin3\"] = pd.cut(\n",
    "    df[\"year_arrived_us\"],\n",
    "    bins=[0, 2005, 2015, 2025],\n",
    "    labels=[\"Before 2005\", \"2005-2015\", \"2015-2023\"],\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "print(f\"\\n3-Bin Categorization:\")\n",
    "for bin_name in df[\"year_arrived_bin3\"].cat.categories:\n",
    "    n_bin = (df[\"year_arrived_bin3\"] == bin_name).sum()\n",
    "    pct = n_bin / len(df) * 100\n",
    "    print(f\"   {bin_name}: {n_bin} ({pct:.1f}%)\")\n",
    "\n",
    "# Missing\n",
    "n_missing_bin = df[\"year_arrived_bin3\"].isna().sum()\n",
    "print(f\"   (Missing): {n_missing_bin}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20693cac",
   "metadata": {},
   "source": [
    "## Step 9: Create Analysis Sample (Final Filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73764e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéØ CREATING FINAL ANALYSIS SAMPLE\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "n_before_final = len(df)\n",
    "\n",
    "# Requirement: valid year_arrived_bin3 (i.e., valid year_arrived_us)\n",
    "df_analysis = df[df[\"year_arrived_bin3\"].notna()].copy()\n",
    "n_final = len(df_analysis)\n",
    "n_excluded_final = n_before_final - n_final\n",
    "\n",
    "print(f\"\\nFinal filtering criterion: Valid year_arrived_bin3\")\n",
    "print(f\"   Records before: {n_before_final}\")\n",
    "print(f\"   Records excluded: {n_excluded_final}\")\n",
    "print(f\"   Analysis sample: {n_final}\")\n",
    "\n",
    "cohort_flow.append({\n",
    "    'Stage': '3. Final analysis sample',\n",
    "    'N': n_final,\n",
    "    'Excluded': n_excluded_final,\n",
    "    'Reason': 'Missing year of arrival bin (missing or out of range data)',\n",
    "    'Cumulative_N': n_final\n",
    "})\n",
    "\n",
    "# Create CV burden measures for analysis sample\n",
    "print(\"\\nüìä Creating composite CV measures for analysis sample:\")\n",
    "cv_conditions = [\"dx_hf\", \"dx_htn\", \"hx_mi\", \"hx_stroke\"]\n",
    "df_analysis[\"cv_burden_count\"] = df_analysis[cv_conditions].sum(axis=1, skipna=True)\n",
    "df_analysis[\"any_cv_condition\"] = (df_analysis[\"cv_burden_count\"] > 0).astype(int)\n",
    "df_analysis[\"major_cv_event\"] = ((df_analysis[\"hx_mi\"] == 1) | (df_analysis[\"hx_stroke\"] == 1)).astype(int)\n",
    "\n",
    "print(f\"   CV burden count created (range: {df_analysis['cv_burden_count'].min():.0f}-{df_analysis['cv_burden_count'].max():.0f})\")\n",
    "print(f\"   Any CV condition: {df_analysis['any_cv_condition'].sum()} ({df_analysis['any_cv_condition'].mean()*100:.1f}%)\")\n",
    "print(f\"   Major CV event: {df_analysis['major_cv_event'].sum()} ({df_analysis['major_cv_event'].mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1133c6e",
   "metadata": {},
   "source": [
    "## Cohort Flow Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9610890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cohort flow table\n",
    "cohort_flow_df = pd.DataFrame(cohort_flow)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COHORT FLOW SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "gt_flow = (\n",
    "    GT(cohort_flow_df)\n",
    "    .tab_header(title=\"Phase 0 Cohort Flow: From Raw Data to Analysis Sample\")\n",
    "    .cols_label(\n",
    "        Stage=\"Stage\",\n",
    "        N=\"Sample Size (N)\",\n",
    "        Excluded=\"Excluded (N)\",\n",
    "        Reason=\"Exclusion Reason\",\n",
    "        Cumulative_N=\"Cumulative N\"\n",
    "    )\n",
    "    .fmt_integer(columns=[\"N\", \"Excluded\", \"Cumulative_N\"])\n",
    ")\n",
    "\n",
    "gt_flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2cf7b6",
   "metadata": {},
   "source": [
    "## Cohort Flow Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebcb7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visual cohort flow\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "stages = cohort_flow_df['Stage'].tolist()\n",
    "sample_sizes = cohort_flow_df['N'].tolist()\n",
    "excluded = cohort_flow_df['Excluded'].tolist()\n",
    "\n",
    "y_positions = np.arange(len(stages))[::-1]  # Reverse for top-to-bottom flow\n",
    "\n",
    "# Plot sample sizes\n",
    "colors = ['#2ecc71' if e == 0 else '#e74c3c' for e in excluded]\n",
    "ax.barh(y_positions, sample_sizes, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "\n",
    "# Add labels\n",
    "for i, (stage, n, ex) in enumerate(zip(stages, sample_sizes, excluded)):\n",
    "    ax.text(n/2, len(stages)-1-i, f'N={int(n)}', \n",
    "            ha='center', va='center', fontweight='bold', fontsize=11, color='white')\n",
    "    if ex > 0:\n",
    "        ax.text(n + max(sample_sizes)*0.02, len(stages)-1-i, f'(-{int(ex)})', \n",
    "                ha='left', va='center', fontsize=10, color='#e74c3c', fontweight='bold')\n",
    "\n",
    "ax.set_yticks(y_positions)\n",
    "ax.set_yticklabels(stages, fontsize=11)\n",
    "ax.set_xlabel('Sample Size (N)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Cohort Flow: Phase 0 Data Cleaning & Selection', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_xlim(0, max(sample_sizes) * 1.15)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Cohort flow visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87099d8e",
   "metadata": {},
   "source": [
    "## Missing Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b282de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MISSING DATA ANALYSIS (Analysis Sample)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Missing data summary\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Variable': df_analysis.columns,\n",
    "    'Missing_N': [df_analysis[col].isna().sum() for col in df_analysis.columns],\n",
    "    'Missing_%': [df_analysis[col].isna().sum() / len(df_analysis) * 100 for col in df_analysis.columns],\n",
    "    'Non_Missing_N': [df_analysis[col].notna().sum() for col in df_analysis.columns],\n",
    "})\n",
    "\n",
    "missing_summary = missing_summary.sort_values('Missing_%', ascending=False)\n",
    "\n",
    "gt_missing = (\n",
    "    GT(missing_summary)\n",
    "    .tab_header(title=\"Missing Data Summary (N=\" + str(len(df_analysis)) + \")\")\n",
    "    .cols_label(\n",
    "        Variable=\"Variable\",\n",
    "        Missing_N=\"Missing (N)\",\n",
    "        Missing_%=\"Missing (%)\",\n",
    "        Non_Missing_N=\"Available (N)\"\n",
    "    )\n",
    "    .fmt_integer(columns=[\"Missing_N\", \"Non_Missing_N\"])\n",
    "    .fmt_number(columns=[\"Missing_%\"], decimals=1)\n",
    ")\n",
    "\n",
    "gt_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e260d0d1",
   "metadata": {},
   "source": [
    "## Data Quality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9659cf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA QUALITY METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n‚úì SAMPLE RETENTION:\")\n",
    "retention_pct = (n_final / n_initial) * 100\n",
    "print(f\"   Original sample: {n_initial}\")\n",
    "print(f\"   Analysis sample: {n_final}\")\n",
    "print(f\"   Retention rate: {retention_pct:.1f}%\")\n",
    "print(f\"   Overall exclusion rate: {100-retention_pct:.1f}%\")\n",
    "\n",
    "print(f\"\\n‚úì DATA COMPLETENESS (Analysis Sample):\")\n",
    "key_vars = ['survey_date', 'age', 'year_arrived_us', 'years_in_us', 'dx_htn', 'hx_mi', 'hx_stroke']\n",
    "for var in key_vars:\n",
    "    if var in df_analysis.columns:\n",
    "        complete_n = df_analysis[var].notna().sum()\n",
    "        complete_pct = complete_n / len(df_analysis) * 100\n",
    "        print(f\"   {var}: {complete_pct:.1f}% complete\")\n",
    "\n",
    "print(f\"\\n‚úì OUTCOME PREVALENCE (Analysis Sample):\")\n",
    "print(f\"   Hypertension: {df_analysis['dx_htn'].sum()} ({df_analysis['dx_htn'].mean()*100:.1f}%)\")\n",
    "print(f\"   Heart attack: {df_analysis['hx_mi'].sum()} ({df_analysis['hx_mi'].mean()*100:.1f}%)\")\n",
    "print(f\"   Stroke: {df_analysis['hx_stroke'].sum()} ({df_analysis['hx_stroke'].mean()*100:.1f}%)\")\n",
    "print(f\"   Any CV condition: {df_analysis['any_cv_condition'].sum()} ({df_analysis['any_cv_condition'].mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ccc9fa",
   "metadata": {},
   "source": [
    "## Data Quality Issues & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4e0221",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA QUALITY ISSUES & RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "issues = []\n",
    "recommendations = []\n",
    "\n",
    "# Check for high missing data\n",
    "for idx, row in missing_summary.iterrows():\n",
    "    if row['Missing_%'] > 50:\n",
    "        issues.append(f\"‚ö†Ô∏è  {row['Variable']}: {row['Missing_%']:.1f}% missing\")\n",
    "        recommendations.append(f\"Consider excluding or imputing {row['Variable']}\")\n",
    "\n",
    "# Check for variables with very low variance\n",
    "if 'dx_hf' in df_analysis.columns:\n",
    "    hf_prev = df_analysis['dx_hf'].mean()\n",
    "    if hf_prev < 0.01:\n",
    "        issues.append(f\"‚ö†Ô∏è  dx_hf: Only {hf_prev*100:.2f}% prevalence (very rare)\")\n",
    "        recommendations.append(\"Consider excluding dx_hf due to insufficient variance\")\n",
    "\n",
    "if retention_pct < 80:\n",
    "    issues.append(f\"‚ö†Ô∏è  Sample retention: {retention_pct:.1f}% (>20% excluded)\")\n",
    "    recommendations.append(\"Review exclusion criteria; consider sensitivity analysis with less stringent criteria\")\n",
    "\n",
    "if len(issues) == 0:\n",
    "    print(\"\\n‚úì NO MAJOR DATA QUALITY ISSUES DETECTED\")\n",
    "else:\n",
    "    print(\"\\nüö® IDENTIFIED ISSUES:\")\n",
    "    for issue in issues:\n",
    "        print(f\"   {issue}\")\n",
    "\n",
    "if recommendations:\n",
    "    print(\"\\nüí° RECOMMENDATIONS FOR PHASE 1:\")\n",
    "    for rec in recommendations:\n",
    "        print(f\"   ‚Ä¢ {rec}\")\n",
    "\n",
    "print(\"\\n‚úì Data quality assessment complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17b19c5",
   "metadata": {},
   "source": [
    "## Summary Statistics (Analysis Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c775138",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS SAMPLE SUMMARY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Demographics\n",
    "print(f\"\\nüë• DEMOGRAPHICS:\")\n",
    "print(f\"   Sample size: {len(df_analysis)}\")\n",
    "print(f\"   Age, mean (SD): {df_analysis['age'].mean():.1f} ({df_analysis['age'].std():.1f})\")\n",
    "print(f\"   Age range: {df_analysis['age'].min():.0f}-{df_analysis['age'].max():.0f}\")\n",
    "\n",
    "# Immigration variables\n",
    "print(f\"\\nüåç IMMIGRATION VARIABLES:\")\n",
    "print(f\"   Years in US, mean (SD): {df_analysis['years_in_us'].mean():.1f} ({df_analysis['years_in_us'].std():.1f})\")\n",
    "print(f\"   Year arrived range: {df_analysis['year_arrived_us'].min():.0f}-{df_analysis['year_arrived_us'].max():.0f}\")\n",
    "print(f\"   Median year arrived: {df_analysis['year_arrived_us'].median():.0f}\")\n",
    "\n",
    "print(f\"\\nüìä YEAR OF ARRIVAL DISTRIBUTION:\")\n",
    "for bin_name in df_analysis[\"year_arrived_bin3\"].cat.categories:\n",
    "    n_bin = (df_analysis[\"year_arrived_bin3\"] == bin_name).sum()\n",
    "    pct = n_bin / len(df_analysis) * 100\n",
    "    print(f\"   {bin_name}: {n_bin} ({pct:.1f}%)\")\n",
    "\n",
    "# Save analysis sample for Phase 0 report\n",
    "df_analysis.to_csv('phase0_analysis_sample.csv', index=False)\n",
    "print(f\"\\n‚úì Analysis sample saved to: phase0_analysis_sample.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
